% Generated by Paperpile. Check out https://paperpile.com for more information.
% BibTeX export options can be customized via Settings -> BibTeX.

@ARTICLE{Nassar2023-oo,
  title    = "The {UCSC} Genome Browser database: 2023 update",
  author   = "Nassar, Luis R and Barber, Galt P and Benet-Pag{\`e}s, Anna and
              Casper, Jonathan and Clawson, Hiram and Diekhans, Mark and
              Fischer, Clay and Gonzalez, Jairo Navarro and Hinrichs, Angie S
              and Lee, Brian T and Lee, Christopher M and Muthuraman, Pranav
              and Nguy, Beagan and Pereira, Tiana and Nejad, Parisa and Perez,
              Gerardo and Raney, Brian J and Schmelter, Daniel and Speir,
              Matthew L and Wick, Brittney D and Zweig, Ann S and Haussler,
              David and Kuhn, Robert M and Haeussler, Maximilian and Kent, W
              James",
  abstract = "The UCSC Genome Browser (https://genome.ucsc.edu) is an omics
              data consolidator, graphical viewer, and general bioinformatics
              resource that continues to serve the community as it enters its
              23rd year. This year has seen an emphasis in clinical data, with
              new tracks and an expanded Recommended Track Sets feature on hg38
              as well as the addition of a single cell track group. SARS-CoV-2
              continues to remain a focus, with regular annotation updates to
              the browser and continued curation of our phylogenetic sequence
              placing tool, hgPhyloPlace, whose tree has now reached over 12M
              sequences. Our GenArk resource has also grown, offering over 2500
              hubs and a system for users to request any absent assemblies. We
              have expanded our bigBarChart display type and created new ways
              to visualize data via bigRmsk and dynseq display. Displaying
              custom annotations is now easier due to our chromAlias system
              which eliminates the requirement for renaming sequence names to
              the UCSC standard. Users involved in data generation may also be
              interested in our new tools and trackDb settings which facilitate
              the creation and display of their custom annotations.",
  journal  = "Nucleic Acids Res.",
  volume   =  51,
  number   = "D1",
  pages    = "D1188--D1195",
  month    =  jan,
  year     =  2023,
  keywords = "scidataflow",
  language = "en"
}

@ARTICLE{Elliott2023-hy,
  title    = "Signing data citations enables data verification and citation
              persistence",
  author   = "Elliott, Michael J and Poelen, Jorrit H and Fortes, Jos{\'e} A B",
  abstract = "Commonly used data citation practices rely on unverifiable
              retrieval methods which are susceptible to content drift, which
              occurs when the data associated with an identifier have been
              allowed to change. Based on our earlier work on reliable dataset
              identifiers, we propose signed citations, i.e., customary data
              citations extended to also include a standards-based, verifiable,
              unique, and fixed-length digital content signature. We show that
              content signatures enable independent verification of the cited
              content and can improve the persistence of the citation. Because
              content signatures are location- and storage-medium-agnostic,
              cited data can be copied to new locations to ensure their
              persistence across current and future storage media and data
              networks. As a result, content signatures can be leveraged to
              help scalably store, locate, access, and independently verify
              content across new and existing data infrastructures. Content
              signatures can also be embedded inside content to create robust,
              distributed knowledge graphs that can be cited using a single
              signed citation. We describe applications of signed citations to
              solve real-world data collection, identification, and citation
              challenges.",
  journal  = "Sci Data",
  volume   =  10,
  number   =  1,
  pages    = "419",
  month    =  jun,
  year     =  2023,
  language = "en"
}

@ARTICLE{Cormier2021-jj,
  title    = "Go Get Data ({GGD}) is a framework that facilitates reproducible
              access to genomic data",
  author   = "Cormier, Michael J and Belyeu, Jonathan R and Pedersen, Brent S
              and Brown, Joseph and K{\"o}ster, Johannes and Quinlan, Aaron R",
  abstract = "The rapid increase in the amount of genomic data provides
              researchers with an opportunity to integrate diverse datasets and
              annotations when addressing a wide range of biological questions.
              However, genomic datasets are deposited on different platforms
              and are stored in numerous formats from multiple genome builds,
              which complicates the task of collecting, annotating,
              transforming, and integrating data as needed. Here, we developed
              Go Get Data (GGD) as a fast, reproducible approach to installing
              standardized data recipes. GGD is available on Github (
              https://gogetdata.github.io/ ), is extendable to other data
              types, and can streamline the complexities typically associated
              with data integration, saving researchers time and improving
              research reproducibility.",
  journal  = "Nat. Commun.",
  volume   =  12,
  number   =  1,
  pages    = "2151",
  month    =  apr,
  year     =  2021,
  language = "en"
}

@ARTICLE{Stolarczyk2020-ux,
  title    = "Refgenie: a reference genome resource manager",
  author   = "Stolarczyk, Micha{\l} and Reuter, Vincent P and Smith, Jason P
              and Magee, Neal E and Sheffield, Nathan C",
  abstract = "BACKGROUND: Reference genome assemblies are essential for
              high-throughput sequencing analysis projects. Typically, genome
              assemblies are stored on disk alongside related resources; e.g.,
              many sequence aligners require the assembly to be indexed. The
              resulting indexes are broadly applicable for downstream analysis,
              so it makes sense to share them. However, there is no simple tool
              to do this. RESULTS: Here, we introduce refgenie, a reference
              genome assembly asset manager. Refgenie makes it easier to
              organize, retrieve, and share genome analysis resources. In
              addition to genome indexes, refgenie can manage any files related
              to reference genomes, including sequences and annotation files.
              Refgenie includes a command line interface and a server
              application that provides a RESTful API, so it is useful for both
              tool development and analysis. CONCLUSIONS: Refgenie streamlines
              sharing genome analysis resources among groups and across
              computing environments. Refgenie is available at
              https://refgenie.databio.org.",
  journal  = "Gigascience",
  volume   =  9,
  number   =  2,
  month    =  feb,
  year     =  2020,
  keywords = "data management; data portability; reference assemblies;
              reference genomes",
  language = "en"
}


@ARTICLE{Elliott2020-le,
  title    = "Toward reliable biodiversity dataset references",
  author   = "Elliott, Michael J and Poelen, Jorrit H and Fortes, Jos{\'e} A B",
  abstract = "No systematic approach has yet been adopted to reliably reference
              and provide access to digital biodiversity datasets. Based on
              accumulated evidence, we argue that location-based identifiers
              such as URLs are not sufficient to ensure long-term data access.
              We introduce a method that uses dedicated data observatories to
              evaluate long-term URL reliability. From March 2019 through May
              2020, we took periodic inventories of the data provided to major
              biodiversity aggregators, including GBIF, iDigBio, DataONE, and
              BHL by accessing the URL-based dataset references from which the
              aggregators retrieve data. Over the period of observation, we
              found that, for the URL-based dataset references available in
              each of the aggregators' data provider registries, 5\% to 70\% of
              URLs were intermittently or consistently unresponsive, 0\% to
              66\% produced unstable content, and 20\% to 75\% became either
              unresponsive or unstable. We propose the use of cryptographic
              hashing to generate content-based identifiers that can reliably
              reference datasets. We show that content-based identifiers
              facilitate decentralized archival and reliable distribution of
              biodiversity datasets to enable long-term accessibility of the
              referenced datasets.",
  journal  = "Ecol. Inform.",
  volume   =  59,
  pages    = "101132",
  month    =  sep,
  year     =  2020,
  keywords = "Biodiversity; Ecological informatics; Information systems;
              Information retrieval"
}



@MISC{Kunze2018-kz,
  title        = "{RFC} 8493: The {BagIt} File Packaging Format (V1.0)",
  booktitle    = "{IETF} Datatracker",
  author       = "Kunze, John A and Littman, Justin and Madden, Liz and
                  Scancella, John and Adams, Chris",
  abstract     = "This document describes BagIt, a set of hierarchical file
                  layout conventions for storage and transfer of arbitrary
                  digital content. A ``bag'' has just enough structure to
                  enclose descriptive metadata ``tags'' and a file ``payload''
                  but does not require knowledge of the payload's internal
                  semantics. This BagIt format is suitable for reliable storage
                  and transfer.",
  month        =  oct,
  year         =  2018,
  howpublished = "",
  note         = "Accessed: 2023-11-15",
  language     = "en"
}

@ARTICLE{Boettiger2015-fy,
  title     = "An introduction to Docker for reproducible research",
  author    = "Boettiger, Carl",
  abstract  = "As computational work becomes more and more integral to many
               aspects of scientific research, computational reproducibility
               has become an issue of increasing importance to computer systems
               researchers and domain scientists alike. Though computational
               reproducibility seems more straight forward than replicating
               physical experiments, the complex and rapidly changing nature of
               computer environments makes being able to reproduce and extend
               such work a serious challenge. In this paper, I explore common
               reasons that code developed for one research project cannot be
               successfully executed or extended by subsequent researchers. I
               review current approaches to these issues, including virtual
               machines and workflow systems, and their limitations. I then
               examine how the popular emerging technology Docker combines
               several areas from systems research - such as operating system
               virtualization, cross-platform portability, modular re-usable
               elements, versioning, and a 'DevOps' philosophy, to address
               these challenges. I illustrate this with several examples of
               Docker use with a focus on the R statistical environment.",
  journal   = "Oper. Syst. Rev.",
  publisher = "Association for Computing Machinery",
  volume    =  49,
  number    =  1,
  pages     = "71--79",
  month     =  jan,
  year      =  2015,
  address   = "New York, NY, USA"
}

@PHDTHESIS{Fielding2000-pe,
  title     = "Architectural styles and the design of network -based software
               architectures",
  author    = "Fielding, Roy Thomas",
  editor    = "Taylor, Richard N",
  abstract  = "The World Wide Web has succeeded in large part because its
               software architecture has been designed to meet the needs of an
               Internet-scale distributed hypermedia system. The Web has been
               iteratively developed over the past ten years through a series
               of modifications to the standards that define its architecture.
               In order to identify those aspects of the Web that needed
               improvement and avoid undesirable modifications, a model for the
               modern Web architecture was needed to guide its design,
               definition, and deployment. Software architecture research
               investigates methods for determining how best to partition a
               system, how components identify and communicate with each other,
               how information is communicated, how elements of a system can
               evolve independently, and how all of the above can be described
               using formal and informal notations. My work is motivated by the
               desire to understand and evaluate the architectural design of
               network-based application software through principled use of
               architectural constraints, thereby obtaining the functional,
               performance, and social properties desired of an architecture.
               An architectural style is a named, coordinated set of
               architectural constraints. This dissertation defines a framework
               for understanding software architecture via architectural styles
               and demonstrates how styles can be used to guide the
               architectural design of network-based application software. A
               survey of architectural styles for network-based applications is
               used to classify styles according to the architectural
               properties they induce on an architecture for distributed
               hypermedia. I then introduce the Representational State Transfer
               (REST) architectural style and describe how REST has been used
               to guide the design and development of the architecture for the
               modern Web. REST emphasizes scalability of component
               interactions, generality of interfaces, independent deployment
               of components, and intermediary components to reduce interaction
               latency, enforce security, and encapsulate legacy systems. I
               describe the software engineering principles guiding REST and
               the interaction constraints chosen to retain those principles,
               contrasting them to the constraints of other architectural
               styles. Finally, I describe the lessons learned from applying
               REST to the design of the Hypertext Transfer Protocol and
               Uniform Resource Identifier standards, and from their subsequent
               deployment in Web client and server software.",
  publisher = "search.proquest.com",
  year      =  2000,
  address   = "Ann Arbor, United States",
  school    = "University of California, Irvine",
  keywords  = "scidataflow",
  language  = "en"
}

@MISC{Allaire2023-js,
  title    = "rmarkdown: Dynamic Documents for {R}",
  author   = "Allaire, J J and Xie, Yihui and Dervieux, Christophe and
              McPherson, Jonathan and Luraschi, Javier and Ushey, Kevin and
              Atkins, Aron and Wickham, Hadley and Cheng, Joe and Chang,
              Winston and Iannone, Richard",
  year     =  2023,
  keywords = "scidataflow"
}

@MISC{Xie2018-ga,
  title     = "{R} Markdown: The Definitive Guide",
  author    = "Xie, Yihui and Allaire, J J and Grolemund, Garrett",
  publisher = "Chapman and Hall/CRC",
  year      =  2018,
  address   = "Boca Raton, Florida",
  keywords  = "scidataflow"
}

@BOOK{Leonelli2016-dv,
  title     = "{Data-Centric} Biology: A Philosophical Study",
  author    = "Leonelli, Sabina",
  abstract  = "In recent decades, there has been a major shift in the way
               researchers process and understand scientific data. Digital
               access to data has revolutionized ways of doing science in the
               biological and biomedical fields, leading to a data-intensive
               approach to research that uses innovative methods to produce,
               store, distribute, and interpret huge amounts of data. In
               Data-Centric Biology, Sabina Leonelli probes the implications of
               these advancements and confronts the questions they pose. Are we
               witnessing the rise of an entirely new scientific epistemology?
               If so, how does that alter the way we study and understand
               life---including ourselves? Leonelli is the first scholar to use
               a study of contemporary data-intensive science to provide a
               philosophical analysis of the epistemology of data. In analyzing
               the rise, internal dynamics, and potential impact of
               data-centric biology, she draws on scholarship across diverse
               fields of science and the humanities---as well as her own
               original empirical material---to pinpoint the conditions under
               which digitally available data can further our understanding of
               life. Bridging the divide between historians, sociologists, and
               philosophers of science, Data-Centric Biology offers a nuanced
               account of an issue that is of fundamental importance to our
               understanding of contemporary scientific practices.",
  publisher = "University of Chicago Press",
  month     =  nov,
  year      =  2016,
  keywords  = "scidataflow",
  language  = "en"
}

@ARTICLE{Edwards2011-ym,
  title    = "Science friction: data, metadata, and collaboration",
  author   = "Edwards, Paul N and Mayernik, Matthew S and Batcheller, Archer L
              and Bowker, Geoffrey C and Borgman, Christine L",
  abstract = "When scientists from two or more disciplines work together on
              related problems, they often face what we call 'science
              friction'. As science becomes more data-driven, collaborative,
              and interdisciplinary, demand increases for interoperability
              among data, tools, and services. Metadata--usually viewed simply
              as 'data about data', describing objects such as books, journal
              articles, or datasets--serve key roles in interoperability. Yet
              we find that metadata may be a source of friction between
              scientific collaborators, impeding data sharing. We propose an
              alternative view of metadata, focusing on its role in an
              ephemeral process of scientific communication, rather than as an
              enduring outcome or product. We report examples of highly useful,
              yet ad hoc, incomplete, loosely structured, and mutable,
              descriptions of data found in our ethnographic studies of several
              large projects in the environmental sciences. Based on this
              evidence, we argue that while metadata products can be powerful
              resources, usually they must be supplemented with metadata
              processes. Metadata-as-process suggests the very large role of
              the ad hoc, the incomplete, and the unfinished in everyday
              scientific work.",
  journal  = "Soc. Stud. Sci.",
  volume   =  41,
  number   =  5,
  pages    = "667--690",
  month    =  oct,
  year     =  2011,
  keywords = "scidataflow",
  language = "en"
}

@ARTICLE{Duke2013-us,
  title     = "The Ethics of Data Sharing and Reuse in Biology",
  author    = "Duke, Clifford S and Porter, John H",
  abstract  = "Abstract. Recent increases in capabilities for gathering,
               storing, accessing, and sharing data are creating corresponding
               opportunities for scientists to use da",
  journal   = "Bioscience",
  publisher = "Oxford Academic",
  volume    =  63,
  number    =  6,
  pages     = "483--489",
  month     =  jun,
  year      =  2013,
  keywords  = "scidataflow",
  language  = "en"
}

@ARTICLE{Tenopir2011-cq,
  title    = "Data sharing by scientists: practices and perceptions",
  author   = "Tenopir, Carol and Allard, Suzie and Douglass, Kimberly and
              Aydinoglu, Arsev Umur and Wu, Lei and Read, Eleanor and Manoff,
              Maribeth and Frame, Mike",
  abstract = "BACKGROUND: Scientific research in the 21st century is more data
              intensive and collaborative than in the past. It is important to
              study the data practices of researchers--data accessibility,
              discovery, re-use, preservation and, particularly, data sharing.
              Data sharing is a valuable part of the scientific method allowing
              for verification of results and extending research from prior
              results. METHODOLOGY/PRINCIPAL FINDINGS: A total of 1329
              scientists participated in this survey exploring current data
              sharing practices and perceptions of the barriers and enablers of
              data sharing. Scientists do not make their data electronically
              available to others for various reasons, including insufficient
              time and lack of funding. Most respondents are satisfied with
              their current processes for the initial and short-term parts of
              the data or research lifecycle (collecting their research data;
              searching for, describing or cataloging, analyzing, and
              short-term storage of their data) but are not satisfied with
              long-term data preservation. Many organizations do not provide
              support to their researchers for data management both in the
              short- and long-term. If certain conditions are met (such as
              formal citation and sharing reprints) respondents agree they are
              willing to share their data. There are also significant
              differences and approaches in data management practices based on
              primary funding agency, subject discipline, age, work focus, and
              world region. CONCLUSIONS/SIGNIFICANCE: Barriers to effective
              data sharing and preservation are deeply rooted in the practices
              and culture of the research process as well as the researchers
              themselves. New mandates for data management plans from NSF and
              other federal agencies and world-wide attention to the need to
              share and preserve data could lead to changes. Large scale
              programs, such as the NSF-sponsored DataNET (including projects
              like DataONE) will both bring attention and resources to the
              issue and make it easier for scientists to apply sound data
              management principles.",
  journal  = "PLoS One",
  volume   =  6,
  number   =  6,
  pages    = "e21101",
  month    =  jun,
  year     =  2011,
  keywords = "scidataflow",
  language = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Borgman2012-zo,
  title     = "The conundrum of sharing research data",
  author    = "Borgman, Christine L",
  abstract  = "We must all accept that science is data and that data are
               science, and thus provide for, and justify the need for the
               support of, much‐improved data curation. (Hanson, Sugden, \&
               Alberts, 2011) Researchers are producing an unprecedented deluge
               of data by using new methods and instrumentation. Others may
               wish to mine these data for new discoveries and innovations.
               However, research data are not readily available as sharing is
               common in only a few fields such as astronomy and genomics. Data
               sharing practices in other fields vary widely. Moreover,
               research data take many forms, are handled in many ways, using
               many approaches, and often are difficult to interpret once
               removed from their initial context. Data sharing is thus a
               conundrum. Four rationales for sharing data are examined,
               drawing examples from the sciences, social sciences, and
               humanities: (1) to reproduce or to verify research, (2) to make
               results of publicly funded research available to the public, (3)
               to enable others to ask new questions of extant data, and (4) to
               advance the state of research and innovation. These rationales
               differ by the arguments for sharing, by beneficiaries, and by
               the motivations and incentives of the many stakeholders
               involved. The challenges are to understand which data might be
               shared, by whom, with whom, under what conditions, why, and to
               what effects. Answers will inform data policy and practice.",
  journal   = "J. Am. Soc. Inf. Sci. Technol.",
  publisher = "Wiley",
  volume    =  63,
  number    =  6,
  pages     = "1059--1078",
  month     =  jun,
  year      =  2012,
  keywords  = "scidataflow",
  language  = "en"
}

@ARTICLE{Reichman2011-up,
  title    = "Challenges and opportunities of open data in ecology",
  author   = "Reichman, O J and Jones, Matthew B and Schildhauer, Mark P",
  abstract = "Ecology is a synthetic discipline benefiting from open access to
              data from the earth, life, and social sciences. Technological
              challenges exist, however, due to the dispersed and heterogeneous
              nature of these data. Standardization of methods and development
              of robust metadata can increase data access but are not
              sufficient. Reproducibility of analyses is also important, and
              executable workflows are addressing this issue by capturing data
              provenance. Sociological challenges, including inadequate rewards
              for sharing data, must also be resolved. The establishment of
              well-curated, federated data repositories will provide a means to
              preserve data while promoting attribution and acknowledgement of
              its use.",
  journal  = "Science",
  volume   =  331,
  number   =  6018,
  pages    = "703--705",
  month    =  feb,
  year     =  2011,
  keywords = "scidataflow",
  language = "en"
}

@ARTICLE{Lowndes2017-ha,
  title    = "Our path to better science in less time using open data science
              tools",
  author   = "Lowndes, Julia S Stewart and Best, Benjamin D and Scarborough,
              Courtney and Afflerbach, Jamie C and Frazier, Melanie R and
              O'Hara, Casey C and Jiang, Ning and Halpern, Benjamin S",
  abstract = "Reproducibility has long been a tenet of science but has been
              challenging to achieve-we learned this the hard way when our old
              approaches proved inadequate to efficiently reproduce our own
              work. Here we describe how several free software tools have
              fundamentally upgraded our approach to collaborative research,
              making our entire workflow more transparent and streamlined. By
              describing specific tools and how we incrementally began using
              them for the Ocean Health Index project, we hope to encourage
              others in the scientific community to do the same-so we can all
              produce better science in less time.",
  journal  = "Nat Ecol Evol",
  volume   =  1,
  number   =  6,
  pages    = "160",
  month    =  may,
  year     =  2017,
  keywords = "scidataflow",
  language = "en"
}

@BOOK{noauthor_undated-ai,
  title     = "Workflows for e-Science",
  publisher = "Springer London",
  keywords  = "scidataflow"
}

@ARTICLE{Ludascher2006-io,
  title     = "Scientific workflow management and the Kepler system",
  author    = "Lud{\"a}scher, Bertram and Altintas, Ilkay and Berkley, Chad and
               Higgins, Dan and Jaeger, Efrat and Jones, Matthew and Lee,
               Edward A and Tao, Jing and Zhao, Yang",
  abstract  = "Many scientific disciplines are now data and information driven,
               and new scientific knowledge is often gained by scientists
               putting together data analysis and knowledge discovery
               `pipelines'. A related trend is that more and more scientific
               communities realize the benefits of sharing their data and
               computational services, and are thus contributing to a
               distributed data and computational community infrastructure
               (a.k.a. `the Grid'). However, this infrastructure is only a
               means to an end and ideally scientists should not be too
               concerned with its existence. The goal is for scientists to
               focus on development and use of what we call scientific
               workflows. These are networks of analytical steps that may
               involve, e.g., database access and querying steps, data analysis
               and mining steps, and many other steps including computationally
               intensive jobs on high-performance cluster computers. In this
               paper we describe characteristics of and requirements for
               scientific workflows as identified in a number of our
               application projects. We then elaborate on Kepler, a particular
               scientific workflow system, currently under development across a
               number of scientific data management projects. We describe some
               key features of Kepler and its underlying Ptolemy II system,
               planned extensions, and areas of future research. Kepler is a
               community-driven, open source project, and we always welcome
               related projects and new contributors to join. Copyright
               \copyright{} 2005 John Wiley \& Sons, Ltd.",
  journal   = "Concurr. Comput.",
  publisher = "Wiley",
  volume    =  18,
  number    =  10,
  pages     = "1039--1065",
  month     =  aug,
  year      =  2006,
  keywords  = "scidataflow",
  language  = "en"
}

@ARTICLE{Piwowar2013-kk,
  title    = "Data reuse and the open data citation advantage",
  author   = "Piwowar, Heather A and Vision, Todd J",
  abstract = "Background. Attribution to the original contributor upon reuse of
              published data is important both as a reward for data creators
              and to document the provenance of research findings. Previous
              studies have found that papers with publicly available datasets
              receive a higher number of citations than similar studies without
              available data. However, few previous analyses have had the
              statistical power to control for the many variables known to
              predict citation rate, which has led to uncertain estimates of
              the ``citation benefit''. Furthermore, little is known about
              patterns in data reuse over time and across datasets. Method and
              Results. Here, we look at citation rates while controlling for
              many known citation predictors and investigate the variability of
              data reuse. In a multivariate regression on 10,555 studies that
              created gene expression microarray data, we found that studies
              that made data available in a public repository received 9\%
              (95\% confidence interval: 5\% to 13\%) more citations than
              similar studies for which the data was not made available. Date
              of publication, journal impact factor, open access status, number
              of authors, first and last author publication history,
              corresponding author country, institution citation history, and
              study topic were included as covariates. The citation benefit
              varied with date of dataset deposition: a citation benefit was
              most clear for papers published in 2004 and 2005, at about 30\%.
              Authors published most papers using their own datasets within two
              years of their first publication on the dataset, whereas data
              reuse papers published by third-party investigators continued to
              accumulate for at least six years. To study patterns of data
              reuse directly, we compiled 9,724 instances of third party data
              reuse via mention of GEO or ArrayExpress accession numbers in the
              full text of papers. The level of third-party data use was high:
              for 100 datasets deposited in year 0, we estimated that 40 papers
              in PubMed reused a dataset by year 2, 100 by year 4, and more
              than 150 data reuse papers had been published by year 5. Data
              reuse was distributed across a broad base of datasets: a very
              conservative estimate found that 20\% of the datasets deposited
              between 2003 and 2007 had been reused at least once by third
              parties. Conclusion. After accounting for other factors affecting
              citation rate, we find a robust citation benefit from open data,
              although a smaller one than previously reported. We conclude
              there is a direct effect of third-party data reuse that persists
              for years beyond the time when researchers have published most of
              the papers reusing their own data. Other factors that may also
              contribute to the citation benefit are considered. We further
              conclude that, at least for gene expression microarray data, a
              substantial fraction of archived datasets are reused, and that
              the intensity of dataset reuse has been steadily increasing since
              2003.",
  journal  = "PeerJ",
  volume   =  1,
  pages    = "e175",
  month    =  oct,
  year     =  2013,
  keywords = "Bibliometrics; Data archiving; Data repositories; Data reuse;
              Gene expression microarray; Incentives; Information science; Open
              data;scidataflow",
  language = "en"
}

@ARTICLE{Wilkinson2016-le,
  title    = "The {FAIR} Guiding Principles for scientific data management and
              stewardship",
  author   = "Wilkinson, Mark D and Dumontier, Michel and Aalbersberg, I
              Jsbrand Jan and Appleton, Gabrielle and Axton, Myles and Baak,
              Arie and Blomberg, Niklas and Boiten, Jan-Willem and da Silva
              Santos, Luiz Bonino and Bourne, Philip E and Bouwman, Jildau and
              Brookes, Anthony J and Clark, Tim and Crosas, Merc{\`e} and
              Dillo, Ingrid and Dumon, Olivier and Edmunds, Scott and Evelo,
              Chris T and Finkers, Richard and Gonzalez-Beltran, Alejandra and
              Gray, Alasdair J G and Groth, Paul and Goble, Carole and Grethe,
              Jeffrey S and Heringa, Jaap and 't Hoen, Peter A C and Hooft, Rob
              and Kuhn, Tobias and Kok, Ruben and Kok, Joost and Lusher, Scott
              J and Martone, Maryann E and Mons, Albert and Packer, Abel L and
              Persson, Bengt and Rocca-Serra, Philippe and Roos, Marco and van
              Schaik, Rene and Sansone, Susanna-Assunta and Schultes, Erik and
              Sengstag, Thierry and Slater, Ted and Strawn, George and Swertz,
              Morris A and Thompson, Mark and van der Lei, Johan and van
              Mulligen, Erik and Velterop, Jan and Waagmeester, Andra and
              Wittenburg, Peter and Wolstencroft, Katherine and Zhao, Jun and
              Mons, Barend",
  abstract = "There is an urgent need to improve the infrastructure supporting
              the reuse of scholarly data. A diverse set of
              stakeholders-representing academia, industry, funding agencies,
              and scholarly publishers-have come together to design and jointly
              endorse a concise and measureable set of principles that we refer
              to as the FAIR Data Principles. The intent is that these may act
              as a guideline for those wishing to enhance the reusability of
              their data holdings. Distinct from peer initiatives that focus on
              the human scholar, the FAIR Principles put specific emphasis on
              enhancing the ability of machines to automatically find and use
              the data, in addition to supporting its reuse by individuals.
              This Comment is the first formal publication of the FAIR
              Principles, and includes the rationale behind them, and some
              exemplar implementations in the community.",
  journal  = "Sci Data",
  volume   =  3,
  pages    = "160018",
  month    =  mar,
  year     =  2016,
  keywords = "scidataflow",
  language = "en"
}

@ARTICLE{Groth2020-mp,
  title     = "{FAIR} data reuse -- the path through data citation",
  author    = "Groth, Paul and Cousijn, Helena and Clark, Tim and Goble, Carole",
  abstract  = "One of the key goals of the FAIR guiding principles is defined
               by its final principle -- to optimize data sets for reuse by
               both humans and machines. To do so, data providers need to
               implement and support consistent machine readable metadata to
               describe their data sets. This can seem like a daunting task for
               data providers, whether it is determining what level of detail
               should be provided in the provenance metadata or figuring out
               what common shared vocabularies should be used. Additionally,
               for existing data sets it is often unclear what steps should be
               taken to enable maximal, appropriate reuse. Data citation
               already plays an important role in making data findable and
               accessible, providing persistent and unique identifiers plus
               metadata on over 16 million data sets. In this paper, we discuss
               how data citation and its underlying infrastructures, in
               particular associated metadata, provide an important pathway for
               enabling FAIR data reuse.",
  journal   = "Data Intell.",
  publisher = "MIT Press",
  volume    =  2,
  number    = "1-2",
  pages     = "78--86",
  month     =  jan,
  year      =  2020,
  keywords  = "scidataflow",
  language  = "en"
}

@ARTICLE{Perrier2017-bu,
  title    = "Research data management in academic institutions: A scoping
              review",
  author   = "Perrier, Laure and Blondal, Erik and Ayala, A Patricia and
              Dearborn, Dylanne and Kenny, Tim and Lightfoot, David and Reka,
              Roger and Thuna, Mindy and Trimble, Leanne and MacDonald, Heather",
  abstract = "OBJECTIVE: The purpose of this study is to describe the volume,
              topics, and methodological nature of the existing research
              literature on research data management in academic institutions.
              MATERIALS AND METHODS: We conducted a scoping review by searching
              forty literature databases encompassing a broad range of
              disciplines from inception to April 2016. We included all study
              types and data extracted on study design, discipline, data
              collection tools, and phase of the research data lifecycle.
              RESULTS: We included 301 articles plus 10 companion reports after
              screening 13,002 titles and abstracts and 654 full-text articles.
              Most articles (85\%) were published from 2010 onwards and
              conducted within the sciences (86\%). More than three-quarters of
              the articles (78\%) reported methods that included interviews,
              cross-sectional, or case studies. Most articles (68\%) included
              the Giving Access to Data phase of the UK Data Archive Research
              Data Lifecycle that examines activities such as sharing data.
              When studies were grouped into five dominant groupings
              (Stakeholder, Data, Library, Tool/Device, and Publication), data
              quality emerged as an integral element. CONCLUSION: Most studies
              relied on self-reports (interviews, surveys) or accounts from an
              observer (case studies) and we found few studies that collected
              empirical evidence on activities amongst data producers,
              particularly those examining the impact of research data
              management interventions. As well, fewer studies examined
              research data management at the early phases of research
              projects. The quality of all research outputs needs attention,
              from the application of best practices in research data
              management studies, to data producers depositing data in
              repositories for long-term use.",
  journal  = "PLoS One",
  volume   =  12,
  number   =  5,
  pages    = "e0178261",
  month    =  may,
  year     =  2017,
  keywords = "scidataflow",
  language = "en"
}

@ARTICLE{Sandve2013-zd,
  title    = "Ten simple rules for reproducible computational research",
  author   = "Sandve, Geir Kjetil and Nekrutenko, Anton and Taylor, James and
              Hovig, Eivind",
  journal  = "PLoS Comput. Biol.",
  volume   =  9,
  number   =  10,
  pages    = "e1003285",
  month    =  oct,
  year     =  2013,
  keywords = "scidataflow",
  language = "en"
}

@ARTICLE{Ram2013-ub,
  title    = "Git can facilitate greater reproducibility and increased
              transparency in science",
  author   = "Ram, Karthik",
  abstract = "BACKGROUND: Reproducibility is the hallmark of good science.
              Maintaining a high degree of transparency in scientific reporting
              is essential not just for gaining trust and credibility within
              the scientific community but also for facilitating the
              development of new ideas. Sharing data and computer code
              associated with publications is becoming increasingly common,
              motivated partly in response to data deposition requirements from
              journals and mandates from funders. Despite this increase in
              transparency, it is still difficult to reproduce or build upon
              the findings of most scientific publications without access to a
              more complete workflow. FINDINGS: Version control systems (VCS),
              which have long been used to maintain code repositories in the
              software industry, are now finding new applications in science.
              One such open source VCS, Git, provides a lightweight yet robust
              framework that is ideal for managing the full suite of research
              outputs such as datasets, statistical code, figures, lab notes,
              and manuscripts. For individual researchers, Git provides a
              powerful way to track and compare versions, retrace errors,
              explore new approaches in a structured manner, while maintaining
              a full audit trail. For larger collaborative efforts, Git and Git
              hosting services make it possible for everyone to work
              asynchronously and merge their contributions at any time, all the
              while maintaining a complete authorship trail. In this paper I
              provide an overview of Git along with use-cases that highlight
              how this tool can be leveraged to make science more reproducible
              and transparent, foster new collaborations, and support novel
              uses.",
  journal  = "Source Code Biol. Med.",
  volume   =  8,
  number   =  1,
  pages    = "7",
  month    =  feb,
  year     =  2013,
  keywords = "scidataflow",
  language = "en"
}

@ARTICLE{Goodman2014-dw,
  title    = "Ten simple rules for the care and feeding of scientific data",
  author   = "Goodman, Alyssa and Pepe, Alberto and Blocker, Alexander W and
              Borgman, Christine L and Cranmer, Kyle and Crosas, Merce and Di
              Stefano, Rosanne and Gil, Yolanda and Groth, Paul and Hedstrom,
              Margaret and Hogg, David W and Kashyap, Vinay and Mahabal, Ashish
              and Siemiginowska, Aneta and Slavkovic, Aleksandra",
  journal  = "PLoS Comput. Biol.",
  volume   =  10,
  number   =  4,
  pages    = "e1003542",
  month    =  apr,
  year     =  2014,
  keywords = "scidataflow",
  language = "en"
}

@ARTICLE{Cunningham2022-vk,
  title    = "Ensembl 2022",
  author   = "Cunningham, Fiona and Allen, James E and Allen, Jamie and
              Alvarez-Jarreta, Jorge and Amode, M Ridwan and Armean, Irina M
              and Austine-Orimoloye, Olanrewaju and Azov, Andrey G and Barnes,
              If and Bennett, Ruth and Berry, Andrew and Bhai, Jyothish and
              Bignell, Alexandra and Billis, Konstantinos and Boddu, Sanjay and
              Brooks, Lucy and Charkhchi, Mehrnaz and Cummins, Carla and Da Rin
              Fioretto, Luca and Davidson, Claire and Dodiya, Kamalkumar and
              Donaldson, Sarah and El Houdaigui, Bilal and El Naboulsi, Tamara
              and Fatima, Reham and Giron, Carlos Garcia and Genez, Thiago and
              Martinez, Jose Gonzalez and Guijarro-Clarke, Cristina and Gymer,
              Arthur and Hardy, Matthew and Hollis, Zoe and Hourlier, Thibaut
              and Hunt, Toby and Juettemann, Thomas and Kaikala, Vinay and Kay,
              Mike and Lavidas, Ilias and Le, Tuan and Lemos, Diana and
              Marug{\'a}n, Jos{\'e} Carlos and Mohanan, Shamika and Mushtaq,
              Aleena and Naven, Marc and Ogeh, Denye N and Parker, Anne and
              Parton, Andrew and Perry, Malcolm and Pili{\v z}ota, Ivana and
              Prosovetskaia, Irina and Sakthivel, Manoj Pandian and Salam,
              Ahamed Imran Abdul and Schmitt, Bianca M and Schuilenburg, Helen
              and Sheppard, Dan and P{\'e}rez-Silva, Jos{\'e} G and Stark,
              William and Steed, Emily and Sutinen, Ky{\"o}sti and Sukumaran,
              Ranjit and Sumathipala, Dulika and Suner, Marie-Marthe and Szpak,
              Michal and Thormann, Anja and Tricomi, Francesca Floriana and
              Urbina-G{\'o}mez, David and Veidenberg, Andres and Walsh, Thomas
              A and Walts, Brandon and Willhoft, Natalie and Winterbottom,
              Andrea and Wass, Elizabeth and Chakiachvili, Marc and Flint,
              Bethany and Frankish, Adam and Giorgetti, Stefano and Haggerty,
              Leanne and Hunt, Sarah E and IIsley, Garth R and Loveland, Jane E
              and Martin, Fergal J and Moore, Benjamin and Mudge, Jonathan M
              and Muffato, Matthieu and Perry, Emily and Ruffier, Magali and
              Tate, John and Thybert, David and Trevanion, Stephen J and Dyer,
              Sarah and Harrison, Peter W and Howe, Kevin L and Yates, Andrew D
              and Zerbino, Daniel R and Flicek, Paul",
  abstract = "Ensembl (https://www.ensembl.org) is unique in its flexible
              infrastructure for access to genomic data and annotation. It has
              been designed to efficiently deliver annotation at scale for all
              eukaryotic life, and it also provides deep comprehensive
              annotation for key species. Genomes representing a greater
              diversity of species are increasingly being sequenced. In
              response, we have focussed our recent efforts on expediting the
              annotation of new assemblies. Here, we report the release of the
              greatest annual number of newly annotated genomes in the history
              of Ensembl via our dedicated Ensembl Rapid Release platform
              (http://rapid.ensembl.org). We have also developed a new method
              to generate comparative analyses at scale for these assemblies
              and, for the first time, we have annotated non-vertebrate
              eukaryotes. Meanwhile, we continually improve, extend and update
              the annotation for our high-value reference vertebrate genomes
              and report the details here. We have a range of specific software
              tools for specific tasks, such as the Ensembl Variant Effect
              Predictor (VEP) and the newly developed interface for the Variant
              Recoder. All Ensembl data, software and tools are freely
              available for download and are accessible programmatically.",
  journal  = "Nucleic Acids Res.",
  volume   =  50,
  number   = "D1",
  pages    = "D988--D995",
  month    =  jan,
  year     =  2022,
  keywords = "BGS;scidataflow",
  language = "en"
}

@BOOK{Buffalo2015-lo,
  title     = "Bioinformatics data skills: Reproducible and Robust Research
               with Open Source Tools",
  author    = "Buffalo, Vince",
  publisher = "O'Reilly Media",
  year      =  2015,
  keywords  = "research\_statement;teaching;F32\_2020;scidataflow"
}

@ARTICLE{Koster2012-iv,
  title    = "Snakemake--a scalable bioinformatics workflow engine",
  author   = "K{\"o}ster, Johannes and Rahmann, Sven",
  abstract = "SUMMARY: Snakemake is a workflow engine that provides a readable
              Python-based workflow definition language and a powerful
              execution environment that scales from single-core workstations
              to compute clusters without modifying the workflow. It is the
              first system to support the use of automatically inferred
              multiple named wildcards (or variables) in input and output
              filenames. AVAILABILITY: http://snakemake.googlecode.com.
              CONTACT: johannes.koester@uni-due.de.",
  journal  = "Bioinformatics",
  volume   =  28,
  number   =  19,
  pages    = "2520--2522",
  month    =  oct,
  year     =  2012,
  keywords = "temporal-covariances-2;BGS;scidataflow",
  language = "en"
}
