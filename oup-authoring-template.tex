%%
%% Copyright 2022 OXFORD UNIVERSITY PRESS
%%
%% This file is part of the 'oup-authoring-template Bundle'.
%% ---------------------------------------------
%%
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%%
%% The list of all files belonging to the 'oup-authoring-template Bundle' is
%% given in the file `manifest.txt'.
%%
%% Template article for OXFORD UNIVERSITY PRESS's document class `oup-authoring-template'
%% with bibliographic references
%%

%%%CONTEMPORARY%%%
\documentclass[unnumsec,webpdf,contemporary,large]{oup-authoring-template}%
%\documentclass[unnumsec,webpdf,contemporary,large,namedate]{oup-authoring-template}% uncomment this line for author year citations and comment the above
%\documentclass[unnumsec,webpdf,contemporary,medium]{oup-authoring-template}
%\documentclass[unnumsec,webpdf,contemporary,small]{oup-authoring-template}

%%%MODERN%%%
%\documentclass[unnumsec,webpdf,modern,large]{oup-authoring-template}
%\documentclass[unnumsec,webpdf,modern,large,namedate]{oup-authoring-template}% uncomment this line for author year citations and comment the above
%\documentclass[unnumsec,webpdf,modern,medium]{oup-authoring-template}
%\documentclass[unnumsec,webpdf,modern,small]{oup-authoring-template}

%%%TRADITIONAL%%%
%\documentclass[unnumsec,webpdf,traditional,large]{oup-authoring-template}
%\documentclass[unnumsec,webpdf,traditional,large,namedate]{oup-authoring-template}% uncomment this line for author year citations and comment the above
%\documentclass[unnumsec,namedate,webpdf,traditional,medium]{oup-authoring-template}
%\documentclass[namedate,webpdf,traditional,small]{oup-authoring-template}

%\onecolumn % for one column layouts

%\usepackage{showframe}


\graphicspath{{Fig/}}

% line numbers
%\usepackage[mathlines, switch]{lineno}
%\usepackage[right]{lineno}

\theoremstyle{thmstyleone}%
\newtheorem{theorem}{Theorem}%  meant for continuous numbers
%%\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
\newtheorem{proposition}[theorem]{Proposition}%
%%\newtheorem{proposition}{Proposition}% to get separate numbers for theorem and proposition etc.
\theoremstyle{thmstyletwo}%
\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%
\theoremstyle{thmstylethree}%
\newtheorem{definition}{Definition}

\begin{document}

\journaltitle{Bioinformatics}
\DOI{DOI HERE}
\copyrightyear{2022}
\pubyear{2019}
\access{Advance Access Publication Date: Day Month Year}
\appnotes{Paper}

\firstpage{1}

%\subtitle{Subject Section}

\title[SciDataFlow]{SciDataFlow: A Tool for Improving the Flow of Data through Science}

\author[1,$\ast$]{Vince Buffalo\ORCID{0000-0003-4510-1609}}

\authormark{Buffalo}

\address[1]{\orgdiv{Department of Integrative Biology}, \orgname{University of California, Berkeley}, \orgaddress{\street{Valley Life Sciences Building }, \postcode{94720}, \state{CA}, \country{USA}}}

\corresp[$\ast$]{Corresponding author. \href{email:vsbuffalo@gmail.com}{vsbuffalo@gmail.com}}

\received{Date}{0}{Year}
\revised{Date}{0}{Year}
\accepted{Date}{0}{Year}

%\editor{Associate Editor: Name}

%\abstract{
%\textbf{Motivation:} .\\
%\textbf{Results:} .\\
%\textbf{Availability:} .\\
%\textbf{Contact:} \href{name@email.com}{name@email.com}\\
%\textbf{Supplementary information:} Supplementary data are available at \textit{Journal Name}
%online.}

\abstract{
\textbf{Motivation:} Managing data and code in open scientific research is complicated by two key problems: large datasets often cannot be stored alongside code in repository platforms like GitHub, and iterative analysis can lead to unnoticed changes to data, increasing the risk that analyses are based on older versions of data. \\
\textbf{Results:} SciDataFlow is a fast, concurrent command-line tool paired with a simple Data Manifest specification that streamlines tracking data changes, uploading data to remote repositories, and pulling in all data necessary to reproduce a computational analysis. \\
\textbf{Availability:} SciDataFlow is available at \url{https://github.com/vsbuffalo/scidataflow}.\\
\textbf{Contact:} \href{vsbuffalo@gmail.com}{vsbuffalo@gmail.com}\\
}
\keywords{data management, reproducible research}

% \boxedtext{
% \begin{itemize}
% \item Key boxed text here.
% \item Key boxed text here.
% \item Key boxed text here.
% \end{itemize}}

\maketitle


\section{Introduction}

The complexity of modern computational scientific projects can be
immense. In genomics, project directories can contain terabytes of raw
and intermediate data scattered across numerous files. These files often
reside alongside processing scripts and analysis code that refines data
into the outputs of a computational analysis: figures, summary
statistics, supplementary files, and ultimately scientific knowledge.
While simply organizing, managing, and sharing large computational projects is
already an arduous task, two problems further complicate computational
research.

First, while code is now often managed using version control systems like Git
and hosted on platforms like GitHub \citep{Ram2013-ub,Buffalo2015-lo}, the
data ---often too large to be hosted on these sites with costs--- is often stored on scientific data
repository sites like FigShare, Zenodo, or Dryad. Unfortunately, this
\emph{data-code divide} bifurcates scientific data from the code that produced
it (i.e. output data) or operates on it (i.e. input data). Reuniting the data
associated with a project would entail downloading and placing it within the
locations expected by the computational pipeline. Overall, this is
time-consuming, limits computational reproducibility, and is a fragile approach
that relies on scientists to carefully document where data is within a project. 
While container platforms such as Docker \citep{Boettiger2015-fy} allow for the sharing of compute environments (e.g. via \href{https://opencontainers.org/}{Open Container Initiative}, and have been used to store and distribute data (e.g. \href{https://oras.land/}{Oras}), using containers for this purpose is a relatively complicated solution. Furthermore, container-based approaches are not well integrated with commonly-used scientific data repository websites such as Zenodo and FigShare.

Second, computational projects often involve rerunning data processing
steps or analyses several times, as software bugs are corrected, input
data is changed by upstream steps or collaborators, or computational
methods are improved. While new software has helped resolve complicated
dependency issues in computational workflows \citep{Koster2012-iv}, the
dynamic evolution of a computational project often leads the researcher
to ask, ``Which data has changed?''. Since scientific results are
conditional on data produced at various steps, this \emph{silent data
modification} issue can lead to erroneous results if the researcher is
not alerted when data is changed.

Here, I introduce \emph{SciDataFlow}, which is both a (1) simple specification
for a \emph{Data Manifest} used to track the data in a project, and (2) a fast,
concurrent command-line tool to register data in the Data Manifest, track when
data changes, retrieve data from and upload data to external repositories, and
store important metadata in a project. The SciDataFlow tool \texttt{sdf}
supports both FigShare and Zenodo, and is designed to be extensible so that
other data repository services with REST interfaces \citep{Fielding2000-pe}
can be added in the future. SciDataFlow differs from tools like Go Get Data \citep{Cormier2021-jj} in that it only manages data resources and does 
not require a executable data creation ``recipe". SciDataFlow is also more 
general than similar useful tools in computational genomics such as Refgenie, 
which manages genomic reference data \citep{Stolarczyk2020-ux}.

\section{The Data Manifest Specification}\label{sec2}

The Data Manifest format is a simple, human and machine-readable
specification in the YAML format. It is designed to be minimal, with three primary components. First, there is a \texttt{files} section containing file sizes and MD5
checksums of the data versions presently ``registered'' in the manifest.
This information is needed to track when local or remotely stored data
has been modified from this registered version. Second, there is a
\texttt{remotes} section containing the minimal sufficient information
to upload and download data from remote data repository services. While
data repository services require secret authentication tokens to access
their interfaces, this private information is automatically stored and
managed outside of the manifest in a user's home directory. The Data Manifest
format is similar but simpler than the BagIt specification \citep[\url{https://datatracker.ietf.org/doc/html/rfc8493}]{Kunze2018-kz},
and is designed to integrate with multiple remote repository platforms through 
a permissive \texttt{remotes} section.

\begin{minipage}{\hsize}
\footnotesize
%\lstset{language=yaml}
\begin{lstlisting}
files:
  data/supplement/figure_1.tsv:
    tracked: true
    md5: 87c1148fa71abf01daceb82d8fbfee53
    size: 993
remotes:
  data/raw: !ZenodoAPI
    name: ancient_dna_analysis
    deposition_id: 8271457
    bucket_url: https://zenodo.org/[...]
metadata:
  title: Ancient DNA Samples
  description: Data to reproduce Joan et al.
\end{lstlisting}
\end{minipage}

Third, the Data Manifest contains a \texttt{metadata} section for
project-wide metadata such as a description. Some additional metadata
about the local user (e.g. their name, email, affiliation) can also be
stored in the user's home directory. All such metadata is automatically
propagated to data repositories that support these metadata fields.

Since SciDataFlow does not manage multiple data versions or backups
(which would increase complexity and disk space usage), the manifest is
plaintext and designed to be tracked by Git. Thus, a simple emergent
feature arises: each Git commit also stores the MD5s of data registered
in the manifest at that time. This allows the user to leverage Git's
powerful history system to track older changes to data.

\section{The SciDataFlow Command Line Tool}\label{sec3}

The SciDataFlow command-line tool \texttt{sdf} is designed to have an
easy-to-use Git-like interface. A new Data Manifest can be initialized
in a project directory with \texttt{sdf\ init}. A common workflow would
be to register a new data file in the manifest (\texttt{sdf\ add}),
indicate this file should be \emph{tracked} by the remote
(\texttt{sdf\ track}), link a remote service to the manifest
(\texttt{sdf\ link}), and push all tracked files to the remote:

\begin{verbatim}
$ sdf add data/supplement/figure_1.tsv
$ sdf track data/supplement/figure_1.tsv
$ sdf link zenodo <access_token> --name ancient_dna_analysis}
$ sdf push
\end{verbatim}

Another common workflow is to clone an existing research project Git
repository and use SciDataFlow to reunite this project with its data.
Since SciDataFlow Data Manifests act as ``recipes" that can be used
to procure data from a variety of repositories, retrieving data for a 
project can be accomplished with one command: \texttt{sdf pull}. This will
concurrently download all files in the manifest into their proper
locations within project subdirectories.

Finally, some data in a project may originate from static URLs. For example,
most reference genomes or annotation data can be downloaded from static links
from Ensembl or the UCSC Genome Browser \citep{Cunningham2022-vk,Nassar2023-oo}. SciDataFlow supports a single
subcommand to download and register files from URLs: \texttt{sdf get link}.
Since large bioinformatics projects may have hundreds of files that need to be
programmatically downloaded, SciDataFlow also has the \texttt{bulk} subcommand,
e.g. \texttt{sdf bulk downloads.tsv --column 2 --header}, which would download
and register all files in the second URL column of this tab-separated value
file, ignoring the first header column.

\section{Discussion}
SciData has numerous possible future extensions. First, since each data file
is stored in the Data Manifest with an MD5 hash, these hashes function as unique 
identifiers. These identifiers can be used to query other repositories
for that data, and their uniqueness can serve as a way to prevent unwanted
data modification \citep{Elliott2020-le,Elliott2023-hy}. Second, 
the \texttt{sdf} command line tool can be easily adapted to first look for 
the data in a local shared data directory, so disk space can be conserved on
shared computing systems.

A core goal of SciDataFlow is to establish new reusable computational
workflow patterns and discourage redundant, isolated efforts. For
example, small tasks like converting a recombination map to a new genome
version or computing summary statistics on a gene set are often
unnecessary and independently duplicated among research groups. In
addition to wasting valuable time, independent re-implementations
introduce multiple points of error and prevent the cumulative
improvement of these important scientific data assets. By making it
effortless to reunite data with code, SciDataFlow promotes modular and
Git-tracked codebases for these tasks, fostering data reuse and
improvement by research communities.
\section{Availability}

SciDataFlow is available at \url{https://github.com/vsbuffalo/scidataflow}, or
can be installed through the Rust Programming Language's crate system with
\texttt{cargo install scidataflow}. SciDataFlow also has extensive documentation 
available at \url{https://vsbuffalo.github.io/scidataflow-doc/}.

\section{Competing interests}
No competing interest is declared.

\section{Author contributions statement}
V.B. wrote the \texttt{sdf} command line tool, documentation, and this manuscript.

\section{Acknowledgments}

The author would like to thank Rasmus Nielsen for support during this work, and Carl Boettiger and two anonymous reviewers for their valuable suggestions. This work is supported by NIH grant R01GM138634 awarded to Rasmus Nielsen.

\bibliographystyle{plain}
\bibliography{reference}
\end{document}
